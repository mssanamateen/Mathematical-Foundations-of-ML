{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e00aae-52f2-4576-854a-d4285f697a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.23.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mlxtend) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mlxtend) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mlxtend) (2.2.3)\n",
      "Collecting scikit-learn>=1.3.1 (from mlxtend)\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mlxtend) (3.10.1)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mlxtend) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=1.3.1->mlxtend) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Downloading mlxtend-0.23.4-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.2/1.4 MB 3.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.2/1.4 MB 1.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.3/1.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.6/1.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.4 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.9/1.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.0/1.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.1/1.4 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.1/1.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/11.1 MB 6.3 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.5/11.1 MB 6.2 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/11.1 MB 5.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.0/11.1 MB 6.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.3/11.1 MB 6.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.5/11.1 MB 5.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.9/11.1 MB 6.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.2/11.1 MB 6.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.7/11.1 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.3/11.1 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.7/11.1 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.1/11.1 MB 6.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.4/11.1 MB 6.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.6/11.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.9/11.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.3/11.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.5/11.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.5/11.1 MB 6.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.9/11.1 MB 6.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.1/11.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.4/11.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.7/11.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.0/11.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.0/11.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.0/11.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.1/11.1 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.2/11.1 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.4/11.1 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.7/11.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.9/11.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.2/11.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.6/11.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.8/11.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.9/11.1 MB 5.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.1 MB 5.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.1 MB 5.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.4/11.1 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.6/11.1 MB 5.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.8/11.1 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.2/11.1 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.7/11.1 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.7/11.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.1 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.1 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.1 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-learn, mlxtend\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "Successfully installed mlxtend-0.23.4 scikit-learn-1.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\~klearn'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c31288a6-4890-4bf1-a143-76686ceb22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Creating a simple dataset\n",
    "data = {\n",
    "    'Study_Hours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Sleep_Hours': [7, 6, 8, 5, 6, 7, 8, 5, 4, 6],\n",
    "    'Breaks_Taken': [5, 3, 2, 4, 1, 3, 2, 5, 4, 2],\n",
    "    'Prev_Score': [50, 55, 65, 60, 70, 80, 75, 85, 90, 95],\n",
    "    'Pass': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]  # 0 = Fail, 1 = Pass\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split features (X) and target (y)\n",
    "X = df.drop(columns=['Pass'])\n",
    "y = df['Pass']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82314f9c-af65-4756-be29-e7895777596a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_splits=5 cannot be greater than the number of members in each class.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:862\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ready_batches\u001b[38;5;241m.\u001b[39mget(block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[1;32m--> 168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Apply Backward Feature Selection\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sfs \u001b[38;5;241m=\u001b[39m SFS(nb_model, \n\u001b[0;32m      6\u001b[0m           k_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# We want to keep the best 2 features\u001b[39;00m\n\u001b[0;32m      7\u001b[0m           forward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Backward selection\u001b[39;00m\n\u001b[0;32m      8\u001b[0m           floating\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \n\u001b[0;32m      9\u001b[0m           scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     10\u001b[0m           cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# 3-fold cross-validation\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m sfs\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Get selected features\u001b[39;00m\n\u001b[0;32m     15\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;28mlist\u001b[39m(sfs\u001b[38;5;241m.\u001b[39mk_feature_idx_)])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mlxtend\\feature_selection\\sequential_feature_selector.py:517\u001b[0m, in \u001b[0;36mSequentialFeatureSelector.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    515\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(k_idx)\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 517\u001b[0m     k_idx, k_score \u001b[38;5;241m=\u001b[39m _calc_score(\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    519\u001b[0m         X_,\n\u001b[0;32m    520\u001b[0m         y,\n\u001b[0;32m    521\u001b[0m         k_idx,\n\u001b[0;32m    522\u001b[0m         groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    523\u001b[0m         feature_groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_groups_,\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubsets_[k] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: k_idx,\n\u001b[0;32m    528\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m: k_score,\n\u001b[0;32m    529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnanmean(k_score),\n\u001b[0;32m    530\u001b[0m     }\n\u001b[0;32m    532\u001b[0m orig_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_ub))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mlxtend\\feature_selection\\utilities.py:100\u001b[0m, in \u001b[0;36m_calc_score\u001b[1;34m(selector, X, y, indices, groups, feature_groups, **fit_params)\u001b[0m\n\u001b[0;32m     97\u001b[0m param_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_params\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sklearn_version \u001b[38;5;241m<\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.4\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selector\u001b[38;5;241m.\u001b[39mcv:\n\u001b[1;32m--> 100\u001b[0m     scores \u001b[38;5;241m=\u001b[39m cross_val_score(\n\u001b[0;32m    101\u001b[0m         selector\u001b[38;5;241m.\u001b[39mest_,\n\u001b[0;32m    102\u001b[0m         X[:, IDX],\n\u001b[0;32m    103\u001b[0m         y,\n\u001b[0;32m    104\u001b[0m         groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    105\u001b[0m         cv\u001b[38;5;241m=\u001b[39mselector\u001b[38;5;241m.\u001b[39mcv,\n\u001b[0;32m    106\u001b[0m         scoring\u001b[38;5;241m=\u001b[39mselector\u001b[38;5;241m.\u001b[39mscorer,\n\u001b[0;32m    107\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    108\u001b[0m         pre_dispatch\u001b[38;5;241m=\u001b[39mselector\u001b[38;5;241m.\u001b[39mpre_dispatch,\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{param_name: fit_params},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     selector\u001b[38;5;241m.\u001b[39mest_\u001b[38;5;241m.\u001b[39mfit(X[:, IDX], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    505\u001b[0m fit_errors_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelimiter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error, n \u001b[38;5;129;01min\u001b[39;00m fit_errors_counter\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m     )\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    100\u001b[0m     {\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HasMethods(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     error_score\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m    141\u001b[0m ):\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    estimator : estimator object implementing 'fit'\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m        The object to use to fit the data.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    X : {array-like, sparse matrix} of shape (n_samples, n_features)\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m        The data to fit. Can be for example a list, or an array.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m        The target variable to try to predict in the case of\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m        supervised learning.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    groups : array-like of shape (n_samples,), default=None\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m        Group labels for the samples used while splitting the dataset into\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m        instance (e.g., :class:`GroupKFold`).\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 1.4\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m            ``groups`` can only be passed if metadata routing is not enabled\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m            via ``sklearn.set_config(enable_metadata_routing=True)``. When routing\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m            is enabled, pass ``groups`` alongside other metadata via the ``params``\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m            argument instead. E.g.:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m            ``cross_validate(..., params={'groups': groups})``.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m    scoring : str, callable, list, tuple, or dict, default=None\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m        Strategy to evaluate the performance of the cross-validated model on\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m        the test set. If `None`, the\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m        :ref:`default evaluation criterion <scoring_api_overview>` of the estimator\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m        is used.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m        If `scoring` represents a single score, one can use:\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m        - a single string (see :ref:`scoring_parameter`);\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m        - a callable (see :ref:`scoring_callable`) that returns a single value.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m        If `scoring` represents multiple scores, one can use:\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m        - a list or tuple of unique strings;\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m        - a callable returning a dictionary where the keys are the metric\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m          names and the values are the metric scores;\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m        - a dictionary with metric names as keys and callables a values.\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m        See :ref:`multimetric_grid_search` for an example.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    cv : int, cross-validation generator or an iterable, default=None\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m        Determines the cross-validation splitting strategy.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m        Possible inputs for cv are:\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m        - None, to use the default 5-fold cross validation,\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m        - int, to specify the number of folds in a `(Stratified)KFold`,\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m        - :term:`CV splitter`,\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m        - An iterable yielding (train, test) splits as arrays of indices.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m        For int/None inputs, if the estimator is a classifier and ``y`` is\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        either binary or multiclass, :class:`StratifiedKFold` is used. In all\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m        other cases, :class:`KFold` is used. These splitters are instantiated\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m        with `shuffle=False` so the splits will be the same across calls.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m        Refer :ref:`User Guide <cross_validation>` for the various\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m        cross-validation strategies that can be used here.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.22\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m            ``cv`` default value if None changed from 3-fold to 5-fold.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m    n_jobs : int, default=None\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m        Number of jobs to run in parallel. Training the estimator and computing\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m        the score are parallelized over the cross-validation splits.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m        for more details.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    verbose : int, default=0\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m        The verbosity level.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    params : dict, default=None\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m        Parameters to pass to the underlying estimator's ``fit``, the scorer,\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m        and the CV splitter.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.4\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    pre_dispatch : int or str, default='2*n_jobs'\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m        Controls the number of jobs that get dispatched during parallel\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m        execution. Reducing this number can be useful to avoid an\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m        explosion of memory consumption when more jobs get dispatched\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m        than CPUs can process. This parameter can be:\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        - An int, giving the exact number of total jobs that are spawned\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m        - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    return_train_score : bool, default=False\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m        Whether to include train scores.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m        Computing training scores is used to get insights on how different\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m        parameter settings impact the overfitting/underfitting trade-off.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m        However computing the scores on the training set can be computationally\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m        expensive and is not strictly required to select the parameters that\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m        yield the best generalization performance.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.19\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.21\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m            Default value was changed from ``True`` to ``False``\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    return_estimator : bool, default=False\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m        Whether to return the estimators fitted on each split.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.20\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m    return_indices : bool, default=False\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m        Whether to return the train-test indices selected for each split.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.3\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    error_score : 'raise' or numeric, default=np.nan\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m        Value to assign to the score if an error occurs in estimator fitting.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m        If set to 'raise', the error is raised.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m        If a numeric value is given, FitFailedWarning is raised.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.20\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;124;03m    scores : dict of float arrays of shape (n_splits,)\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m        Array of scores of the estimator for each run of the cross validation.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m        A dict of arrays containing the score/time arrays for each scorer is\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m        returned. The possible keys for this ``dict`` are:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m        ``test_score``\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m            The score array for test scores on each cv split.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m            Suffix ``_score`` in ``test_score`` changes to a specific\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m            metric like ``test_r2`` or ``test_auc`` if there are\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m            multiple scoring metrics in the scoring parameter.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03m        ``train_score``\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m            The score array for train scores on each cv split.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m            Suffix ``_score`` in ``train_score`` changes to a specific\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m            metric like ``train_r2`` or ``train_auc`` if there are\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m            multiple scoring metrics in the scoring parameter.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m            This is available only if ``return_train_score`` parameter\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m            is ``True``.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m        ``fit_time``\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m            The time for fitting the estimator on the train\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m            set for each cv split.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m        ``score_time``\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m            The time for scoring the estimator on the test set for each\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m            cv split. (Note time for scoring on the train set is not\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m            included even if ``return_train_score`` is set to ``True``\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m        ``estimator``\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m            The estimator objects for each cv split.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m            This is available only if ``return_estimator`` parameter\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m            is set to ``True``.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m        ``indices``\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m            The train/test positional indices for each cv split. A dictionary\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m            is returned where the keys are either `\"train\"` or `\"test\"`\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;124;03m            and the associated values are a list of integer-dtyped NumPy\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;124;03m            arrays with the indices. Available only if `return_indices=True`.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    cross_val_score : Run cross-validation for single metric evaluation.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    cross_val_predict : Get predictions from each split of cross-validation for\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m        diagnostic purposes.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m        loss function.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m    >>> from sklearn import datasets, linear_model\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.model_selection import cross_validate\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.metrics import make_scorer\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.metrics import confusion_matrix\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.svm import LinearSVC\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m    >>> diabetes = datasets.load_diabetes()\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    >>> X = diabetes.data[:150]\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    >>> y = diabetes.target[:150]\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m    >>> lasso = linear_model.Lasso()\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m    Single metric evaluation using ``cross_validate``\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    >>> cv_results = cross_validate(lasso, X, y, cv=3)\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    >>> sorted(cv_results.keys())\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    ['fit_time', 'score_time', 'test_score']\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m    >>> cv_results['test_score']\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m    array([0.3315057 , 0.08022103, 0.03531816])\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    Multiple metric evaluation using ``cross_validate``\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    (please refer the ``scoring`` parameter doc for more information)\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    >>> scores = cross_validate(lasso, X, y, cv=3,\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    ...                         scoring=('r2', 'neg_mean_squared_error'),\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    ...                         return_train_score=True)\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m    >>> print(scores['test_neg_mean_squared_error'])\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    [-3635.5... -3573.3... -6114.7...]\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    >>> print(scores['train_r2'])\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    [0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    343\u001b[0m     _check_groups_routing_disabled(groups)\n\u001b[0;32m    345\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m indexable(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterable):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch the tasks and return the results.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    iterable : iterable\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        Iterable containing tuples of (delayed_function, args, kwargs) that should\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m        be consumed.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    results : list\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m        List of results of the tasks.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     config \u001b[38;5;241m=\u001b[39m get_config()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:873\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    870\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_effective_n_jobs\n\u001b[0;32m    871\u001b[0m big_batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m n_jobs\n\u001b[1;32m--> 873\u001b[0m islice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mislice(iterator, big_batch_size))\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(islice) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:59\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterable):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch the tasks and return the results.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;124;03m    iterable : iterable\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        Iterable containing tuples of (delayed_function, args, kwargs) that should\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m        be consumed.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    results : list\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m        List of results of the tasks.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     config \u001b[38;5;241m=\u001b[39m get_config()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    100\u001b[0m     {\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HasMethods(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     error_score\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m    141\u001b[0m ):\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    estimator : estimator object implementing 'fit'\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m        The object to use to fit the data.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    X : {array-like, sparse matrix} of shape (n_samples, n_features)\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m        The data to fit. Can be for example a list, or an array.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m        The target variable to try to predict in the case of\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m        supervised learning.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    groups : array-like of shape (n_samples,), default=None\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m        Group labels for the samples used while splitting the dataset into\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m        instance (e.g., :class:`GroupKFold`).\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 1.4\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m            ``groups`` can only be passed if metadata routing is not enabled\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m            via ``sklearn.set_config(enable_metadata_routing=True)``. When routing\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m            is enabled, pass ``groups`` alongside other metadata via the ``params``\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m            argument instead. E.g.:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m            ``cross_validate(..., params={'groups': groups})``.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m    scoring : str, callable, list, tuple, or dict, default=None\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m        Strategy to evaluate the performance of the cross-validated model on\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m        the test set. If `None`, the\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m        :ref:`default evaluation criterion <scoring_api_overview>` of the estimator\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m        is used.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m        If `scoring` represents a single score, one can use:\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m        - a single string (see :ref:`scoring_parameter`);\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m        - a callable (see :ref:`scoring_callable`) that returns a single value.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m        If `scoring` represents multiple scores, one can use:\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m        - a list or tuple of unique strings;\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m        - a callable returning a dictionary where the keys are the metric\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m          names and the values are the metric scores;\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m        - a dictionary with metric names as keys and callables a values.\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m        See :ref:`multimetric_grid_search` for an example.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    cv : int, cross-validation generator or an iterable, default=None\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m        Determines the cross-validation splitting strategy.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m        Possible inputs for cv are:\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m        - None, to use the default 5-fold cross validation,\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m        - int, to specify the number of folds in a `(Stratified)KFold`,\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m        - :term:`CV splitter`,\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m        - An iterable yielding (train, test) splits as arrays of indices.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m        For int/None inputs, if the estimator is a classifier and ``y`` is\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        either binary or multiclass, :class:`StratifiedKFold` is used. In all\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m        other cases, :class:`KFold` is used. These splitters are instantiated\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m        with `shuffle=False` so the splits will be the same across calls.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m        Refer :ref:`User Guide <cross_validation>` for the various\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m        cross-validation strategies that can be used here.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.22\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m            ``cv`` default value if None changed from 3-fold to 5-fold.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m    n_jobs : int, default=None\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m        Number of jobs to run in parallel. Training the estimator and computing\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m        the score are parallelized over the cross-validation splits.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m        for more details.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    verbose : int, default=0\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m        The verbosity level.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    params : dict, default=None\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m        Parameters to pass to the underlying estimator's ``fit``, the scorer,\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m        and the CV splitter.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.4\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    pre_dispatch : int or str, default='2*n_jobs'\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m        Controls the number of jobs that get dispatched during parallel\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m        execution. Reducing this number can be useful to avoid an\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m        explosion of memory consumption when more jobs get dispatched\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m        than CPUs can process. This parameter can be:\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        - An int, giving the exact number of total jobs that are spawned\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m        - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    return_train_score : bool, default=False\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m        Whether to include train scores.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m        Computing training scores is used to get insights on how different\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m        parameter settings impact the overfitting/underfitting trade-off.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m        However computing the scores on the training set can be computationally\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m        expensive and is not strictly required to select the parameters that\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m        yield the best generalization performance.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.19\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.21\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m            Default value was changed from ``True`` to ``False``\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    return_estimator : bool, default=False\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m        Whether to return the estimators fitted on each split.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.20\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m    return_indices : bool, default=False\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m        Whether to return the train-test indices selected for each split.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.3\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    error_score : 'raise' or numeric, default=np.nan\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m        Value to assign to the score if an error occurs in estimator fitting.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m        If set to 'raise', the error is raised.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m        If a numeric value is given, FitFailedWarning is raised.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.20\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;124;03m    scores : dict of float arrays of shape (n_splits,)\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m        Array of scores of the estimator for each run of the cross validation.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m        A dict of arrays containing the score/time arrays for each scorer is\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m        returned. The possible keys for this ``dict`` are:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m        ``test_score``\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m            The score array for test scores on each cv split.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m            Suffix ``_score`` in ``test_score`` changes to a specific\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m            metric like ``test_r2`` or ``test_auc`` if there are\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m            multiple scoring metrics in the scoring parameter.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03m        ``train_score``\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m            The score array for train scores on each cv split.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m            Suffix ``_score`` in ``train_score`` changes to a specific\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m            metric like ``train_r2`` or ``train_auc`` if there are\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m            multiple scoring metrics in the scoring parameter.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m            This is available only if ``return_train_score`` parameter\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m            is ``True``.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m        ``fit_time``\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m            The time for fitting the estimator on the train\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m            set for each cv split.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m        ``score_time``\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m            The time for scoring the estimator on the test set for each\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m            cv split. (Note time for scoring on the train set is not\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m            included even if ``return_train_score`` is set to ``True``\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m        ``estimator``\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m            The estimator objects for each cv split.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m            This is available only if ``return_estimator`` parameter\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m            is set to ``True``.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m        ``indices``\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m            The train/test positional indices for each cv split. A dictionary\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m            is returned where the keys are either `\"train\"` or `\"test\"`\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;124;03m            and the associated values are a list of integer-dtyped NumPy\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;124;03m            arrays with the indices. Available only if `return_indices=True`.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    cross_val_score : Run cross-validation for single metric evaluation.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    cross_val_predict : Get predictions from each split of cross-validation for\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m        diagnostic purposes.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m        loss function.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m    >>> from sklearn import datasets, linear_model\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.model_selection import cross_validate\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.metrics import make_scorer\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.metrics import confusion_matrix\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.svm import LinearSVC\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m    >>> diabetes = datasets.load_diabetes()\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    >>> X = diabetes.data[:150]\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    >>> y = diabetes.target[:150]\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m    >>> lasso = linear_model.Lasso()\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m    Single metric evaluation using ``cross_validate``\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    >>> cv_results = cross_validate(lasso, X, y, cv=3)\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    >>> sorted(cv_results.keys())\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    ['fit_time', 'score_time', 'test_score']\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m    >>> cv_results['test_score']\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m    array([0.3315057 , 0.08022103, 0.03531816])\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    Multiple metric evaluation using ``cross_validate``\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    (please refer the ``scoring`` parameter doc for more information)\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    >>> scores = cross_validate(lasso, X, y, cv=3,\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    ...                         scoring=('r2', 'neg_mean_squared_error'),\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    ...                         return_train_score=True)\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m    >>> print(scores['test_neg_mean_squared_error'])\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    [-3635.5... -3573.3... -6114.7...]\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    >>> print(scores['train_r2'])\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    [0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    343\u001b[0m     _check_groups_routing_disabled(groups)\n\u001b[0;32m    345\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m indexable(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:352\u001b[0m, in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n_splits, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of folds must be of Integral type. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m was passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_splits, \u001b[38;5;28mtype\u001b[39m(n_splits))\n\u001b[0;32m    351\u001b[0m     )\n\u001b[1;32m--> 352\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_splits)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_splits \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-fold cross-validation requires at least one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m train/test split by setting n_splits=2 or more,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m got n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits)\n\u001b[0;32m    359\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:85\u001b[0m, in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m        The testing set indices for that split.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     87\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe groups parameter is ignored by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m     89\u001b[0m         )\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups\u001b[38;5;241m=\u001b[39mgroups)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:733\u001b[0m, in \u001b[0;36m_iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStratifiedKFold\u001b[39;00m(_BaseKFold):\n\u001b[0;32m    687\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stratified K-Fold cross-validator.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \n\u001b[0;32m    689\u001b[0m \u001b[38;5;124;03m    Provides train/test indices to split data in train/test sets.\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m    This cross-validation object is a variation of KFold that returns\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;124;03m    stratified folds. The folds are made by preserving the percentage of\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    samples for each class.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <stratified_k_fold>`.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \n\u001b[0;32m    697\u001b[0m \u001b[38;5;124;03m    For visualisation of cross-validation behaviour and\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;124;03m    comparison between common scikit-learn split methods\u001b[39;00m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;124;03m    refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\u001b[39;00m\n\u001b[0;32m    700\u001b[0m \n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;124;03m    n_splits : int, default=5\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;124;03m        Number of folds. Must be at least 2.\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \n\u001b[0;32m    706\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.22\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;124;03m            ``n_splits`` default value changed from 3 to 5.\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    shuffle : bool, default=False\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;124;03m        Whether to shuffle each class's samples before splitting into batches.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m        Note that the samples within each split will not be shuffled.\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \n\u001b[0;32m    713\u001b[0m \u001b[38;5;124;03m    random_state : int, RandomState instance or None, default=None\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;124;03m        When `shuffle` is True, `random_state` affects the ordering of the\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;124;03m        indices, which controls the randomness of each fold for each class.\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m        Otherwise, leave `random_state` as `None`.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;124;03m        Pass an int for reproducible output across multiple function calls.\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03m        See :term:`Glossary <random_state>`.\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \n\u001b[0;32m    720\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;124;03m    >>> import numpy as np\u001b[39;00m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.model_selection import StratifiedKFold\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;124;03m    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;124;03m    >>> y = np.array([0, 0, 1, 1])\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;124;03m    >>> skf = StratifiedKFold(n_splits=2)\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03m    >>> skf.get_n_splits(X, y)\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;124;03m    >>> print(skf)\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;124;03m    StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;124;03m    >>> for i, (train_index, test_index) in enumerate(skf.split(X, y)):\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;124;03m    ...     print(f\"Fold {i}:\")\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m \u001b[38;5;124;03m    ...     print(f\"  Train: index={train_index}\")\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;124;03m    ...     print(f\"  Test:  index={test_index}\")\u001b[39;00m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m    Fold 0:\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;124;03m      Train: index=[1 3]\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;124;03m      Test:  index=[0 2]\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;124;03m    Fold 1:\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;124;03m      Train: index=[0 2]\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03m      Test:  index=[1 3]\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \n\u001b[0;32m    742\u001b[0m \u001b[38;5;124;03m    Notes\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;124;03m    -----\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;124;03m    The implementation is designed to:\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \n\u001b[0;32m    746\u001b[0m \u001b[38;5;124;03m    * Generate test sets such that all contain the same distribution of\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m      classes, or as close as possible.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;124;03m      ``y = [1, 0]`` should not change the indices generated.\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;124;03m    * Preserve order dependencies in the dataset ordering, when\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m      ``shuffle=False``: all samples from class k in some test set were\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m      contiguous in y, or separated in y by samples from classes other than k.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    * Generate test sets where the smallest and largest differ by at most one\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03m      sample.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m    .. versionchanged:: 0.22\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;124;03m        The previous implementation did not follow the last constraint.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;124;03m    RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\u001b[39;00m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m*\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    765\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(n_splits\u001b[38;5;241m=\u001b[39mn_splits, shuffle\u001b[38;5;241m=\u001b[39mshuffle, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:695\u001b[0m, in \u001b[0;36m_make_test_folds\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStratifiedKFold\u001b[39;00m(_BaseKFold):\n\u001b[0;32m    687\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stratified K-Fold cross-validator.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \n\u001b[0;32m    689\u001b[0m \u001b[38;5;124;03m    Provides train/test indices to split data in train/test sets.\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m    This cross-validation object is a variation of KFold that returns\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;124;03m    stratified folds. The folds are made by preserving the percentage of\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    samples for each class.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \n\u001b[1;32m--> 695\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <stratified_k_fold>`.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \n\u001b[0;32m    697\u001b[0m \u001b[38;5;124;03m    For visualisation of cross-validation behaviour and\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;124;03m    comparison between common scikit-learn split methods\u001b[39;00m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;124;03m    refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\u001b[39;00m\n\u001b[0;32m    700\u001b[0m \n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;124;03m    n_splits : int, default=5\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;124;03m        Number of folds. Must be at least 2.\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \n\u001b[0;32m    706\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.22\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;124;03m            ``n_splits`` default value changed from 3 to 5.\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    shuffle : bool, default=False\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;124;03m        Whether to shuffle each class's samples before splitting into batches.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m        Note that the samples within each split will not be shuffled.\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \n\u001b[0;32m    713\u001b[0m \u001b[38;5;124;03m    random_state : int, RandomState instance or None, default=None\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;124;03m        When `shuffle` is True, `random_state` affects the ordering of the\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;124;03m        indices, which controls the randomness of each fold for each class.\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m        Otherwise, leave `random_state` as `None`.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;124;03m        Pass an int for reproducible output across multiple function calls.\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03m        See :term:`Glossary <random_state>`.\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \n\u001b[0;32m    720\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;124;03m    >>> import numpy as np\u001b[39;00m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.model_selection import StratifiedKFold\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;124;03m    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;124;03m    >>> y = np.array([0, 0, 1, 1])\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;124;03m    >>> skf = StratifiedKFold(n_splits=2)\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03m    >>> skf.get_n_splits(X, y)\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;124;03m    >>> print(skf)\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;124;03m    StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;124;03m    >>> for i, (train_index, test_index) in enumerate(skf.split(X, y)):\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;124;03m    ...     print(f\"Fold {i}:\")\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;124;03m    ...     print(f\"  Train: index={train_index}\")\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;124;03m    ...     print(f\"  Test:  index={test_index}\")\u001b[39;00m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m    Fold 0:\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;124;03m      Train: index=[1 3]\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;124;03m      Test:  index=[0 2]\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;124;03m    Fold 1:\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;124;03m      Train: index=[0 2]\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03m      Test:  index=[1 3]\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \n\u001b[0;32m    742\u001b[0m \u001b[38;5;124;03m    Notes\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;124;03m    -----\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;124;03m    The implementation is designed to:\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \n\u001b[0;32m    746\u001b[0m \u001b[38;5;124;03m    * Generate test sets such that all contain the same distribution of\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m      classes, or as close as possible.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;124;03m      ``y = [1, 0]`` should not change the indices generated.\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;124;03m    * Preserve order dependencies in the dataset ordering, when\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m      ``shuffle=False``: all samples from class k in some test set were\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m      contiguous in y, or separated in y by samples from classes other than k.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    * Generate test sets where the smallest and largest differ by at most one\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03m      sample.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m    .. versionchanged:: 0.22\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;124;03m        The previous implementation did not follow the last constraint.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;124;03m    RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\u001b[39;00m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m*\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    765\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(n_splits\u001b[38;5;241m=\u001b[39mn_splits, shuffle\u001b[38;5;241m=\u001b[39mshuffle, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n",
      "\u001b[1;31mValueError\u001b[0m: n_splits=5 cannot be greater than the number of members in each class."
     ]
    }
   ],
   "source": [
    "# Initialize Naïve Bayes model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Apply Backward Feature Selection\n",
    "sfs = SFS(nb_model, \n",
    "          k_features=2,  # We want to keep the best 2 features\n",
    "          forward=False,  # Backward selection\n",
    "          floating=False,  \n",
    "          scoring='accuracy', \n",
    "          cv=3)  # 3-fold cross-validation\n",
    "\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = list(X.columns[list(sfs.k_feature_idx_)])\n",
    "print(\"Selected Features (Backward Wrapper):\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14524a07-4ab9-4caa-86bc-feafa08f2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Train Naïve Bayes\n",
    "nb_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Naïve Bayes Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9de3af-3422-459a-879d-01b5e601e4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
